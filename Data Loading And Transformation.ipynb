{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ddec8c-a604-4fa2-902d-6b3176232598",
     "showTitle": true,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls FileStore/tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97b42037-4869-4d71-86f3-9f1842aa55c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('dbfs:/FileStore/tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aed7712-a5cc-47b4-85aa-1f16837e9456",
     "showTitle": true,
     "title": "EDA & Transformation"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option('multiline', 'true').json('dbfs:/FileStore/tables/Streaming_History_Audio_2019_2024.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47bc9015-f9bf-48a4-bc7e-65882328bfca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d4bd555-ab83-4d0e-bf2e-035b6184159e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# No. of Columns in df dataset\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b22f3138-9754-4415-a5eb-599cc2b390f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Total no of records in dataset.\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f768c06-f9b2-43ec-b2d7-81bd9fd7cb9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e320e1-1f9d-4a1a-8c5b-f02c629dc6ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('master_metadata_album_album_name' ,'album_name')\\\n",
    "    .withColumnRenamed('master_metadata_album_artist_name', 'artist_name')\\\n",
    "        .withColumnRenamed('master_metadata_track_name', 'track_name')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d144b90c-0e8a-47b3-b6c5-56e13e1febca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select('platform').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b733d2c-d5f8-4148-ae49-85be222c66cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, when\n",
    "df = df.withColumn('platform',\n",
    "                   when(lower('platform').contains('windows'), 'Windows')\\\n",
    "                       .when(lower('platform').contains('android'), 'Android')\\\n",
    "                           .otherwise(None)\n",
    "                           )\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "923d9e93-26ea-44ca-8313-4357b55590aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a new dataset by considering only required features.\n",
    "sp_df = df.select(['album_name', 'artist_name', 'track_name', 'ms_played', 'platform', 'reason_end', 'reason_start', 'shuffle', 'skipped', 'ts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e8f182-cd24-430c-bf16-198e05b846ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(sp_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97527bb1-60a4-445b-8ce4-8d18d837a4bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# No. of records in dataset\n",
    "sp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39f45a3d-893a-4fc9-a32e-e54da563f94a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sp_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba1d60e-4358-4103-b3f1-7d4d2eeff519",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Displaying the count of NULL values in each Column.\n",
    "from pyspark.sql.functions import col, when, count, coalesce\n",
    "df_null_count = sp_df.select([count(when(col(c).isNull(),c)).alias(c) for c in sp_df.columns])\n",
    "df_null_count.show(10)\n",
    "# A Dictonary with columns and count of null values.\n",
    "df_null_count_dict = {c : sp_df.filter(col(c).isNull()).count() for c in sp_df.columns}\n",
    "df_null_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "788dbbe2-a0db-4a91-a2df-2fa6a9324b49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sp_df = sp_df.dropna(subset=['album_name', 'artist_name', 'track_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de96f7cd-a3de-4ba0-b9b2-a99dd7d3b070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replacing the NULL values to False\n",
    "#Method 1 - Using WHEN function.\n",
    "sp_df = sp_df.withColumn('skipped', when(col('skipped').isNull(), False).otherwise(col('skipped')))\n",
    "\n",
    "#Method 2 - Using COALESCE function.\n",
    "# sp_df = sp_df.withColumn('skipped', coalesce(col('skipped'), lit(False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ace88db-e4c1-433e-8c10-67b92cb5ca2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count, coalesce\n",
    "df_null_count = sp_df.select([count(when(col(c).isNull(),c)).alias(c) for c in sp_df.columns])\n",
    "df_null_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a351120c-c71b-4d18-8b4c-5779a55cd7de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sp_df = sp_df.withColumn('date', col('ts').cast('date'))\n",
    "sp_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cb884f7-6c73-4d23-81cc-ea7dea272606",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "sp_df = sp_df.withColumn('year', date_format('date', 'y'))\\\n",
    "        .withColumn('month', date_format('date', 'MMMM'))\\\n",
    "            .withColumn('day', date_format('date', 'EEEE'))\n",
    "sp_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df88cab9-5b6b-46fd-be03-8cc32bcb1b0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "sp_df = sp_df.withColumn('month_num', date_format(col('ts'), 'M'))\n",
    "sp_df = sp_df.withColumn('day_num',\n",
    "                 when(col('day')=='Sunday', 1)\\\n",
    "                     .when(col('day')=='Monday', 2)\\\n",
    "                         .when(col('day')=='Tuesday', 3)\\\n",
    "                             .when(col('day')=='Wednesday', 4)\\\n",
    "                                 .when(col('day')=='Thursday', 5)\\\n",
    "                                     .when(col('day')=='Friday', 6)\\\n",
    "                                         .when(col('day')=='Saturday', 7)\\\n",
    "                                             .otherwise(None))\n",
    "sp_df = sp_df.withColumn('hour', date_format(col('ts'), 'H'))\n",
    "sp_df = sp_df.withColumn('year', col('year').cast(IntegerType()))\\\n",
    "    .withColumn('month_num', col('month_num').cast(IntegerType()))\\\n",
    "        .withColumn('hour', col('hour').cast(IntegerType()))\n",
    "sp_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea3839b-5db0-4580-97c6-f5f7d34753ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbca5e16-1bc1-46b1-91bb-4a6d80d702b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sp_df.groupBy('reason_end').count().orderBy('count', ascending = False).show()\n",
    "sp_df.groupBy('shuffle').count().orderBy('count', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63658d8b-e52e-44db-96f4-5e7b7ab29bd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sp_df.write.saveAsTable('spotify', format='delta', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c7ef6fa-350c-49c6-9865-a64efa6b4704",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 210016576716913,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Loading And Transformation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
